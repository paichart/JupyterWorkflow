{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dced095a-f55e-43b6-b80f-d3674ea93d7b",
   "metadata": {},
   "source": [
    "# Llama Agents Example\n",
    "## By praison, June 27, 2024\n",
    "### https://www.youtube.com/watch?v=nEQCpSd5mx8&list=PLjjyqNsKVN8lRgfJwqSWlPh_MSJZUPw2y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d785e3d-dedd-4bb1-9f9f-9cdc6732c693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install llama-agents llama-index-agent-openai llama-index-embeddings-openai llama-index-program-openai\n",
    "export OPENAI_API_KEY=xxxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e0d64c-1bd5-40f1-9178-495fe6da55b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2365b-82af-4825-9921-7844c136e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    AgentOrchestrator,\n",
    "    ControlPlaneServer,\n",
    "    LocalLauncher,\n",
    "    SimpleMessageQueue,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Create tool\n",
    "def get_the_secret_fact() -> str:\n",
    "    \"\"\"Returns the secret fact.\"\"\"\n",
    "    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n",
    "\n",
    "\n",
    "tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
    "\n",
    "# Create Agents\n",
    "worker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\n",
    "worker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\n",
    "agent1 = worker1.as_agent()\n",
    "agent2 = worker2.as_agent()\n",
    "\n",
    "# Create Key components\n",
    "message_queue = SimpleMessageQueue()\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=AgentOrchestrator(llm=OpenAI()),\n",
    ")\n",
    "agent_server_1 = AgentService(\n",
    "    agent=agent1,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Useful for getting the secret fact.\",\n",
    "    service_name=\"secret_fact_agent\",\n",
    ")\n",
    "\n",
    "agent_server_2 = AgentService(\n",
    "    agent=agent2,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Useful for getting random dumb facts.\",\n",
    "    service_name=\"dumb_fact_agent\",\n",
    ")\n",
    "\n",
    "# Llama Kickoff\n",
    "launcher = LocalLauncher([agent_server_1, agent_server_2], control_plane, message_queue)\n",
    "result = launcher.launch_single(\"What is the secret fact?\")\n",
    "\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a9bb1-0dfc-4fe6-8e64-9013116efdd7",
   "metadata": {},
   "source": [
    "## Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc911a-9b5b-4f99-b0c5-c3d1dfcafec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    HumanService,\n",
    "    AgentOrchestrator,\n",
    "    CallableMessageConsumer,\n",
    "    ControlPlaneServer,\n",
    "    ServerLauncher,\n",
    "    SimpleMessageQueue,\n",
    "    QueueMessage,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "# create an agent\n",
    "def get_the_secret_fact() -> str:\n",
    "    \"\"\"Returns the secret fact.\"\"\"\n",
    "    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n",
    "\n",
    "\n",
    "tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
    "\n",
    "worker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\n",
    "worker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\n",
    "agent1 = worker1.as_agent()\n",
    "agent2 = worker2.as_agent()\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "queue_client = message_queue.client\n",
    "\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=queue_client,\n",
    "    orchestrator=AgentOrchestrator(llm=OpenAI()),\n",
    ")\n",
    "agent_server_1 = AgentService(\n",
    "    agent=agent1,\n",
    "    message_queue=queue_client,\n",
    "    description=\"Useful for getting the secret fact.\",\n",
    "    service_name=\"secret_fact_agent\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8002,\n",
    ")\n",
    "agent_server_2 = AgentService(\n",
    "    agent=agent2,\n",
    "    message_queue=queue_client,\n",
    "    description=\"Useful for getting random dumb facts.\",\n",
    "    service_name=\"dumb_fact_agent\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8003,\n",
    ")\n",
    "human_service = HumanService(\n",
    "    message_queue=queue_client,\n",
    "    description=\"Answers queries about math.\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8004,\n",
    ")\n",
    "\n",
    "\n",
    "# additional human consumer\n",
    "def handle_result(message: QueueMessage) -> None:\n",
    "    print(\"Got result:\", message.data)\n",
    "\n",
    "\n",
    "human_consumer = CallableMessageConsumer(handler=handle_result, message_type=\"human\")\n",
    "\n",
    "# launch it\n",
    "launcher = ServerLauncher(\n",
    "    [agent_server_1, agent_server_2, human_service],\n",
    "    control_plane,\n",
    "    message_queue,\n",
    "    additional_consumers=[human_consumer],\n",
    ")\n",
    "\n",
    "launcher.launch_servers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969ba39-2587-4615-8d31-27f36aaa2c10",
   "metadata": {},
   "source": [
    "## LLama Agents monitor\n",
    "### Required above server setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e95d23-c11d-4af8-a941-db8fac276de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama-agents monitor --control-plane-url http://127.0.0.1:8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f41ce-29b3-4aab-b984-35d62031fc4d",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f06eb6-d54c-43c5-9b4e-0708546e1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    PipelineOrchestrator,\n",
    "    ServiceComponent,\n",
    "    LocalLauncher,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "\n",
    "# create an agent\n",
    "def get_the_secret_fact() -> str:\n",
    "    \"\"\"Returns the secret fact.\"\"\"\n",
    "    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n",
    "\n",
    "\n",
    "tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
    "\n",
    "worker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\n",
    "# worker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\n",
    "agent1 = worker1.as_agent()\n",
    "agent2 = OpenAIAgent.from_tools(\n",
    "    [], system_prompt=\"Repeat the input with a silly fact added.\"\n",
    ")  # worker2.as_agent()\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "\n",
    "agent_server_1 = AgentService(\n",
    "    agent=agent1,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Useful for getting the secret fact.\",\n",
    "    service_name=\"secret_fact_agent\",\n",
    ")\n",
    "agent_server_2 = AgentService(\n",
    "    agent=agent2,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Useful for getting random dumb facts.\",\n",
    "    service_name=\"dumb_fact_agent\",\n",
    ")\n",
    "\n",
    "agent_component_1 = ServiceComponent.from_service_definition(agent_server_1)\n",
    "agent_component_2 = ServiceComponent.from_service_definition(agent_server_2)\n",
    "\n",
    "pipeline = QueryPipeline(\n",
    "    chain=[\n",
    "        agent_component_1,\n",
    "        agent_component_2,\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_orchestrator = PipelineOrchestrator(pipeline)\n",
    "\n",
    "control_plane = ControlPlaneServer(message_queue, pipeline_orchestrator)\n",
    "\n",
    "# launch it\n",
    "launcher = LocalLauncher([agent_server_1, agent_server_2], control_plane, message_queue)\n",
    "result = launcher.launch_single(\"What is the secret fact?\")\n",
    "\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416905c4-dd18-4635-99bc-e65800ae599e",
   "metadata": {},
   "source": [
    "## Heirarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d94e64d-b370-479c-ac6d-b596a9e9f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    PipelineOrchestrator,\n",
    "    ServiceComponent,\n",
    "    LocalLauncher,\n",
    ")\n",
    "from llama_agents.tools import AgentServiceTool\n",
    "\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "\n",
    "# create an agent\n",
    "def get_the_secret_fact() -> str:\n",
    "    \"\"\"Returns the secret fact.\"\"\"\n",
    "    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n",
    "\n",
    "\n",
    "tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
    "\n",
    "worker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\n",
    "agent1 = worker1.as_agent()\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "\n",
    "agent1_server = AgentService(\n",
    "    agent=agent1,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Useful for getting the secret fact.\",\n",
    "    service_name=\"secret_fact_agent\",\n",
    ")\n",
    "\n",
    "agent1_server_tool = AgentServiceTool.from_service_definition(\n",
    "    message_queue=message_queue, service_definition=agent1_server.service_definition\n",
    ")\n",
    "\n",
    "agent2 = OpenAIAgent.from_tools(\n",
    "    [agent1_server_tool],\n",
    "    system_prompt=\"Perform the task, return the result as well as a funny joke.\",\n",
    ")\n",
    "\n",
    "agent2_server = AgentService(\n",
    "    agent=agent2,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Useful for telling funny jokes.\",\n",
    "    service_name=\"dumb_fact_agent\",\n",
    ")\n",
    "\n",
    "agent2_component = ServiceComponent.from_service_definition(agent2_server)\n",
    "\n",
    "pipeline = QueryPipeline(chain=[agent2_component])\n",
    "\n",
    "pipeline_orchestrator = PipelineOrchestrator(pipeline)\n",
    "\n",
    "control_plane = ControlPlaneServer(message_queue, pipeline_orchestrator)\n",
    "\n",
    "# launch it\n",
    "launcher = LocalLauncher([agent1_server, agent2_server], control_plane, message_queue)\n",
    "result = launcher.launch_single(\"What is the secret fact?\")\n",
    "\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83085346-dd25-4b8e-bde2-04c2c0bff570",
   "metadata": {},
   "source": [
    "## Human in the Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485668b1-7bb5-4909-80d7-4b457c1af04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    HumanService,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    PipelineOrchestrator,\n",
    "    ServiceComponent,\n",
    "    LocalLauncher,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.query_pipeline import RouterComponent, QueryPipeline\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.selectors import PydanticSingleSelector\n",
    "\n",
    "\n",
    "# create an agent\n",
    "def get_the_secret_fact() -> str:\n",
    "    \"\"\"Returns the secret fact.\"\"\"\n",
    "    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n",
    "\n",
    "\n",
    "tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "\n",
    "worker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\n",
    "agent = worker.as_agent()\n",
    "agent_service = AgentService(\n",
    "    agent=agent,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Useful for getting the secret fact.\",\n",
    "    service_name=\"secret_fact_agent\",\n",
    ")\n",
    "agent_component = ServiceComponent.from_service_definition(agent_service)\n",
    "\n",
    "human_service = HumanService(\n",
    "    message_queue=message_queue, description=\"Answers queries about math.\"\n",
    ")\n",
    "human_component = ServiceComponent.from_service_definition(human_service)\n",
    "\n",
    "pipeline = QueryPipeline(\n",
    "    chain=[\n",
    "        RouterComponent(\n",
    "            selector=PydanticSingleSelector.from_defaults(llm=OpenAI()),\n",
    "            choices=[agent_service.description, human_service.description],\n",
    "            components=[agent_component, human_component],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_orchestrator = PipelineOrchestrator(pipeline)\n",
    "\n",
    "control_plane = ControlPlaneServer(message_queue, pipeline_orchestrator)\n",
    "\n",
    "# launch it\n",
    "launcher = LocalLauncher([agent_service, human_service], control_plane, message_queue)\n",
    "result = launcher.launch_single(\"What is 1 + 2 + 3 + 4 + 5?\")\n",
    "\n",
    "print(f\"Result: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
